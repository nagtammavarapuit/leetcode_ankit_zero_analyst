{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-6M5RIHNP:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>zero_analyst</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x25042a77310>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.master('local').appName('zero_analyst').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MySQL connection properties\n",
    "mysql_host = \"localhost\"\n",
    "mysql_port = 3306\n",
    "mysql_database = \"zero_analyst\"\n",
    "mysql_username = \"root\"\n",
    "mysql_password = \"mypassword\"\n",
    "\n",
    "# JDBC URL\n",
    "jdbc_url = f\"jdbc:mysql://{mysql_host}:{mysql_port}/{mysql_database}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-----------+--------+\n",
      "|employee_id|           name| department|  salary|\n",
      "+-----------+---------------+-----------+--------+\n",
      "|          1|       John Doe|Engineering|63000.00|\n",
      "|          2|     Jane Smith|Engineering|55000.00|\n",
      "|          3|Michael Johnson|Engineering|64000.00|\n",
      "|          4|    Emily Davis|  Marketing|58000.00|\n",
      "|          5|    Chris Brown|  Marketing|56000.00|\n",
      "|          6|    Emma Wilson|  Marketing|59000.00|\n",
      "|          7|       Alex Lee|      Sales|58000.00|\n",
      "|          8|    Sarah Adams|      Sales|58000.00|\n",
      "|          9|     Ryan Clark|      Sales|61000.00|\n",
      "|         11|           zara|         it|63000.00|\n",
      "+-----------+---------------+-----------+--------+\n",
      "\n",
      "+-----------+---------------+-----------+--------+---+\n",
      "|employee_id|           name| department|  salary|dnk|\n",
      "+-----------+---------------+-----------+--------+---+\n",
      "|          3|Michael Johnson|Engineering|64000.00|  1|\n",
      "|          6|    Emma Wilson|  Marketing|59000.00|  1|\n",
      "|          9|     Ryan Clark|      Sales|61000.00|  1|\n",
      "|         11|           zara|         it|63000.00|  1|\n",
      "+-----------+---------------+-----------+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 01 Write the SQL query to find the second highest salary   SELECT * FROM employees01 \n",
    "employees01 = spark.read.format(\"jdbc\").option(\"url\", jdbc_url).option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", 'employees01').option(\"user\", mysql_username).option(\"password\", mysql_password).load()\n",
    "\n",
    "employees01.show()\n",
    "# -- m1\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col,dense_rank\n",
    "ws=Window.partitionBy('department').orderBy(col('salary').desc())\n",
    "\n",
    "employees01.withColumn('dnk',dense_rank().over(ws)).filter(col('dnk')<2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------+\n",
      "|OrderID| OrderDate|TotalAmount|\n",
      "+-------+----------+-----------+\n",
      "|      1|2023-01-15|     150.50|\n",
      "|      2|2023-02-20|     200.75|\n",
      "|      3|2023-02-28|     300.25|\n",
      "|      4|2023-03-10|     180.00|\n",
      "|      5|2023-04-05|     250.80|\n",
      "+-------+----------+-----------+\n",
      "\n",
      "+--------+-------+\n",
      "|ReturnID|OrderID|\n",
      "+--------+-------+\n",
      "|     104|      1|\n",
      "|     101|      2|\n",
      "|     105|      3|\n",
      "|     102|      4|\n",
      "|     103|      5|\n",
      "+--------+-------+\n",
      "\n",
      "+----------------+---------------+\n",
      "|month(OrderDate)|count(ReturnID)|\n",
      "+----------------+---------------+\n",
      "|               1|              1|\n",
      "|               3|              1|\n",
      "|               4|              1|\n",
      "|               2|              2|\n",
      "+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 02 write an SQL query to calculate the total  numbers of returned orders for each month select * from orders02; select * from returns02;\n",
    "orders02 = spark.read.format(\"jdbc\").option(\"url\", jdbc_url).option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", 'orders02').option(\"user\", mysql_username).option(\"password\", mysql_password).load()\n",
    "returns02 = spark.read.format(\"jdbc\").option(\"url\", jdbc_url).option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", 'returns02').option(\"user\", mysql_username).option(\"password\", mysql_password).load()\n",
    "from pyspark.sql.functions import count,month\n",
    "orders02.show()\n",
    "returns02.show()\n",
    "\n",
    "orders02.join(returns02,'OrderID','left').groupby(month('OrderDate')).agg(count(col('ReturnID'))).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------+-------------+\n",
      "|product_id|        product_name|      category|quantity_sold|\n",
      "+----------+--------------------+--------------+-------------+\n",
      "|         1|  Samsung Galaxy S20|   Electronics|          100|\n",
      "|         2| Apple iPhone 12 Pro|   Electronics|          150|\n",
      "|         3|  Sony PlayStation 5|   Electronics|           80|\n",
      "|         4|    Nike Air Max 270|      Clothing|          200|\n",
      "|         5|Adidas Ultraboost 20|      Clothing|          200|\n",
      "|         6|Levis Mens 501 Jeans|      Clothing|           90|\n",
      "|         7|Instant Pot Duo 7...|Home & Kitchen|          180|\n",
      "|         8|Keurig K-Classic ...|Home & Kitchen|          130|\n",
      "|         9|iRobot Roomba 675...|Home & Kitchen|          130|\n",
      "|        10|Breville Compact ...|Home & Kitchen|           90|\n",
      "|        11|Dyson V11 Animal ...|Home & Kitchen|           90|\n",
      "+----------+--------------------+--------------+-------------+\n",
      "\n",
      "+----------+--------------------+--------------+-------------+---+\n",
      "|product_id|        product_name|      category|quantity_sold|dnk|\n",
      "+----------+--------------------+--------------+-------------+---+\n",
      "|         4|    Nike Air Max 270|      Clothing|          200|  1|\n",
      "|         5|Adidas Ultraboost 20|      Clothing|          200|  1|\n",
      "|         2| Apple iPhone 12 Pro|   Electronics|          150|  1|\n",
      "|         7|Instant Pot Duo 7...|Home & Kitchen|          180|  1|\n",
      "+----------+--------------------+--------------+-------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 03 Write SQL query to find the top-selling products in each category   SELECT * FROm products03;\n",
    "products03 = spark.read.format(\"jdbc\").option(\"url\", jdbc_url).option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", 'products03').option(\"user\", mysql_username).option(\"password\", mysql_password).load()\n",
    "\n",
    "products03.show()\n",
    "ws=Window.partitionBy('category').orderBy(col('quantity_sold').desc())\n",
    "products03.withColumn('dnk',dense_rank().over(ws)).filter(col('dnk')==1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------+-----+----------------+\n",
      "|   category|        product|user_id|spend|transaction_date|\n",
      "+-----------+---------------+-------+-----+----------------+\n",
      "|  appliance|   refrigerator|    165|  246|      2021-12-26|\n",
      "|  appliance|   refrigerator|    123|  300|      2022-03-02|\n",
      "|  appliance| washingmachine|    123|  220|      2022-03-02|\n",
      "|electronics|         vacuum|    178|  152|      2022-04-05|\n",
      "|electronics|wirelessheadset|    156|  250|      2022-07-08|\n",
      "|electronics|             TV|    145|  189|      2022-07-15|\n",
      "| Television|             TV|    165|  129|      2022-07-15|\n",
      "| Television|             TV|    163|  129|      2022-07-15|\n",
      "| Television|             TV|    141|  129|      2022-07-15|\n",
      "|       toys|          Ben10|    145|  189|      2022-07-15|\n",
      "|       toys|          Ben10|    145|  189|      2022-07-15|\n",
      "|       toys|           yoyo|    165|  129|      2022-07-15|\n",
      "|       toys|           yoyo|    163|  129|      2022-07-15|\n",
      "|       toys|           yoyo|    141|  129|      2022-07-15|\n",
      "|       toys|           yoyo|    145|  189|      2022-07-15|\n",
      "|electronics|         vacuum|    145|  189|      2022-07-15|\n",
      "+-----------+---------------+-------+-----+----------------+\n",
      "\n",
      "+-----------+\n",
      "|   category|\n",
      "+-----------+\n",
      "|       toys|\n",
      "|electronics|\n",
      "+-----------+\n",
      "\n",
      "+---------------+-----------+----------+\n",
      "|        product|   category|sum(spend)|\n",
      "+---------------+-----------+----------+\n",
      "|           yoyo|       toys|       576|\n",
      "|          Ben10|       toys|       378|\n",
      "|         vacuum|electronics|       341|\n",
      "|wirelessheadset|electronics|       250|\n",
      "+---------------+-----------+----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 04 Find the top 2 products in the top 2 categories based on spend amount? SELECT * FROM orders04;\n",
    "orders04 = spark.read.format(\"jdbc\").option(\"url\", jdbc_url).option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", 'orders04').option(\"user\", mysql_username).option(\"password\", mysql_password).load()\n",
    "orders04.show()\n",
    "from pyspark.sql.functions import *\n",
    "#orders04.printSchema()\n",
    "#ws=Window.partitionBy('category').orderBy(col('spend').desc())\n",
    "cat_df=orders04.groupby('category').agg(sum('spend')).orderBy(col('sum(spend)').desc()).select('category').limit(2)\n",
    "cat_df.show()\n",
    "\n",
    "cat_df.join(orders04,'category','inner').groupBy('product','category').agg(sum(col('spend'))).orderBy(col('sum(spend)').desc()).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------------+\n",
      "|customer_id|         name|            email|\n",
      "+-----------+-------------+-----------------+\n",
      "|          1|     John Doe| john@example.com|\n",
      "|          2|   Jane Smith| jane@example.com|\n",
      "|          3|Alice Johnson|alice@example.com|\n",
      "|          4|        Sam B|   sb@example.com|\n",
      "|          5|   John Smith|    j@example.com|\n",
      "+-----------+-------------+-----------------+\n",
      "\n",
      "+--------+-----------+----------+------+\n",
      "|order_id|customer_id|order_date|amount|\n",
      "+--------+-----------+----------+------+\n",
      "|       1|          1|2024-03-05| 50.00|\n",
      "|       2|          2|2024-03-10| 75.00|\n",
      "|       5|          4|2024-04-02| 45.00|\n",
      "|       5|          2|2024-04-02| 45.00|\n",
      "|       3|          4|2024-04-15|100.00|\n",
      "|       4|          1|2024-04-01| 60.00|\n",
      "|       5|          5|2024-04-02| 45.00|\n",
      "|       5|          5|2024-09-02| 45.00|\n",
      "|       6|          1|2024-04-05|100.00|\n",
      "|       6|          1|2025-01-10| 10.00|\n",
      "|       7|          2|2025-01-07| 10.00|\n",
      "+--------+-----------+----------+------+\n",
      "\n",
      "+-----------+-------------+-----------------+\n",
      "|customer_id|         name|            email|\n",
      "+-----------+-------------+-----------------+\n",
      "|          3|Alice Johnson|alice@example.com|\n",
      "|          5|   John Smith|    j@example.com|\n",
      "|          4|        Sam B|   sb@example.com|\n",
      "+-----------+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 06 -- who has not done purchase in last month (orders)  SELECT * FROM customers06;SELECT * FROM orders06;\n",
    "customers06 = spark.read.format(\"jdbc\").option(\"url\", jdbc_url).option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", 'customers06').option(\"user\", mysql_username).option(\"password\", mysql_password).load()\n",
    "orders06 = spark.read.format(\"jdbc\").option(\"url\", jdbc_url).option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", 'orders06').option(\"user\", mysql_username).option(\"password\", mysql_password).load()\n",
    "customers06.show()\n",
    "orders06.show()\n",
    "\n",
    "from pyspark.sql.functions import col, year, month, current_date, expr\n",
    "df_current = spark.createDataFrame([(1,)], [\"dummy\"]) \\\n",
    "                  .withColumn(\"current_year\", year(current_date())) \\\n",
    "                  .withColumn(\"last_month\", expr(\"IF(month(current_date())=1, 12, month(current_date())-1)\"))\n",
    "\n",
    "current_year = df_current.collect()[0][\"current_year\"]\n",
    "last_month = df_current.collect()[0][\"last_month\"]\n",
    "\n",
    "# Filter orders from the last month\n",
    "last_month_orders = orders06.filter(\n",
    "    (year(col(\"order_date\")) == current_year) & \n",
    "    (month(col(\"order_date\")) == last_month)\n",
    ").select(\"customer_id\").distinct()\n",
    "\n",
    "# Find customers who have NOT made a purchase in the last month using LEFT ANTI JOIN\n",
    "inactive_customers = customers06.join(last_month_orders, \"customer_id\", \"left_anti\")\n",
    "\n",
    "# Show results\n",
    "inactive_customers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------+--------+\n",
      "|emp_id|          name|department|  salary|\n",
      "+------+--------------+----------+--------+\n",
      "|     1|      John Doe|   Finance|60000.00|\n",
      "|     2|    Jane Smith|   Finance|65000.00|\n",
      "|     2|    Jane Smith|   Finance|65000.00|\n",
      "|     9| Lisa Anderson|     Sales|63000.00|\n",
      "|     9| Lisa Anderson|     Sales|63000.00|\n",
      "|     9| Lisa Anderson|     Sales|63000.00|\n",
      "|    10|Kevin Martinez|     Sales|61000.00|\n",
      "+------+--------------+----------+--------+\n",
      "\n",
      "+-------------+-----------+\n",
      "|         name|count(name)|\n",
      "+-------------+-----------+\n",
      "|Lisa Anderson|          3|\n",
      "|   Jane Smith|          2|\n",
      "+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 07   How would you identify duplicate entries in a SQL in given table employees columns are  emp_id, name, department, salary:\n",
    "employees07 = spark.read.format(\"jdbc\").option(\"url\", jdbc_url).option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", 'employees07').option(\"user\", mysql_username).option(\"password\", mysql_password).load()\n",
    "employees07.show()\n",
    "\n",
    "employees07.groupby('name').agg(count('name')).filter(col('count(name)')>1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-----+\n",
      "|product_id|product_name|  category|price|\n",
      "+----------+------------+----------+-----+\n",
      "|         1|   Product A|Category 1|10.00|\n",
      "|         2|   Product B|Category 2|15.00|\n",
      "|         3|   Product C|Category 1|20.00|\n",
      "|         4|   Product D|Category 3|25.00|\n",
      "+----------+------------+----------+-----+\n",
      "\n",
      "+-------+----------+----------+--------+\n",
      "|sale_id|product_id| sale_date|quantity|\n",
      "+-------+----------+----------+--------+\n",
      "|      1|         1|2023-09-15|       5|\n",
      "|      2|         2|2023-10-20|       3|\n",
      "|      3|         1|2024-01-05|       2|\n",
      "|      4|         3|2024-02-10|       4|\n",
      "|      5|         4|2023-12-03|       1|\n",
      "|      6|         1|2024-12-30|      10|\n",
      "|      7|         3|2024-11-30|     100|\n",
      "|      8|         2|2024-10-30|       5|\n",
      "|      9|         3|2024-09-30|       5|\n",
      "+-------+----------+----------+--------+\n",
      "\n",
      "+----------+------------+----------+-----+\n",
      "|product_id|product_name|  category|price|\n",
      "+----------+------------+----------+-----+\n",
      "|         1|   Product A|Category 1|10.00|\n",
      "|         1|   Product A|Category 1|10.00|\n",
      "|         4|   Product D|Category 3|25.00|\n",
      "|         3|   Product C|Category 1|20.00|\n",
      "|         2|   Product B|Category 2|15.00|\n",
      "+----------+------------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 08   Write a  query to find all products that have not been sold in the last six months. \n",
    "# Return the product_id, product_name, category,  and price of these products. SELECT * FROM products08;SELECT * FROM sales08;\n",
    "products08 = spark.read.format(\"jdbc\").option(\"url\", jdbc_url).option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", 'products08').option(\"user\", mysql_username).option(\"password\", mysql_password).load()\n",
    "sales08 = spark.read.format(\"jdbc\").option(\"url\", jdbc_url).option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", 'sales08').option(\"user\", mysql_username).option(\"password\", mysql_password).load()\n",
    "products08.show()\n",
    "sales08.show()\n",
    "from pyspark.sql.functions import col, current_date, date_sub\n",
    "six_months_ago = date_sub(current_date(), 6 * 30)  # Approximate 6 months as 180 days\n",
    "\n",
    "# Left join products with sales to find unsold products or those last sold more than 6 months ago\n",
    "unsold_products = products08.join(sales08, \"product_id\", \"left\") \\\n",
    "    .filter((col(\"sale_date\").isNull()) | (col(\"sale_date\") < six_months_ago)) \\\n",
    "    .select(products08[\"*\"])  # Selecting required columns\n",
    "\n",
    "# Show results\n",
    "unsold_products.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|CustomerID|CustomerName|\n",
      "+----------+------------+\n",
      "|         1|        John|\n",
      "|         2|        Emma|\n",
      "|         3|     Michael|\n",
      "|         4|         Ben|\n",
      "|         5|        John|\n",
      "+----------+------------+\n",
      "\n",
      "+----------+----------+-----------+------------+\n",
      "|PurchaseID|CustomerID|ProductName|PurchaseDate|\n",
      "+----------+----------+-----------+------------+\n",
      "|       100|         1|     iPhone|  2024-01-01|\n",
      "|       101|         1|    MacBook|  2024-01-20|\n",
      "|       102|         1|    Airpods|  2024-03-10|\n",
      "|       103|         2|       iPad|  2024-03-05|\n",
      "|       104|         2|     iPhone|  2024-03-15|\n",
      "|       105|         3|    MacBook|  2024-03-20|\n",
      "|       106|         3|    Airpods|  2024-03-25|\n",
      "|       107|         4|     iPhone|  2024-03-22|\n",
      "|       108|         4|    Airpods|  2024-03-29|\n",
      "|       110|         5|    Airpods|  2024-02-29|\n",
      "|       109|         5|     iPhone|  2024-03-22|\n",
      "+----------+----------+-----------+------------+\n",
      "\n",
      "+----------+------------+\n",
      "|CustomerID|CustomerName|\n",
      "+----------+------------+\n",
      "|         1|        John|\n",
      "|         4|         Ben|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 09  write a SQL query to find customers who  bought Airpods after purchasing an iPhone. SELECT * FROM customers09; SELECT * FROM purchases09;\n",
    "customers09 = spark.read.format(\"jdbc\").option(\"url\", jdbc_url).option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", 'customers09').option(\"user\", mysql_username).option(\"password\", mysql_password).load()\n",
    "purchases09 = spark.read.format(\"jdbc\").option(\"url\", jdbc_url).option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", 'purchases09').option(\"user\", mysql_username).option(\"password\", mysql_password).load()\n",
    "customers09.show()\n",
    "purchases09.show()\n",
    "\n",
    "p1 = purchases09.alias(\"p1\").withColumnRenamed('ProductName','p1_ProductName').withColumnRenamed('PurchaseDate','p1_PurchaseDate').filter(col(\"ProductName\") == \"iPhone\")\n",
    "p2 = purchases09.alias(\"p2\").withColumnRenamed('ProductName','p2_ProductName').withColumnRenamed('PurchaseDate','p2_PurchaseDate').filter(col(\"ProductName\") == \"Airpods\")\n",
    "spark.conf.set(\"spark.sql.analyzer.failAmbiguousSelfJoin\", \"false\")\n",
    "#p1.show()\n",
    "purchases_joined = customers09.join(p1,'CustomerID','inner').join(p2,'CustomerID','inner').filter(p1.p1_PurchaseDate < p2.p2_PurchaseDate).select('CustomerID','CustomerName')\n",
    "purchases_joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+----------+--------+\n",
      "|EmployeeID|FirstName|LastName|Department|  Salary|\n",
      "+----------+---------+--------+----------+--------+\n",
      "|         1|     John|     Doe|   Finance|75000.00|\n",
      "|         2|     Jane|   Smith|        HR|60000.00|\n",
      "|         3|  Michael| Johnson|        IT|45000.00|\n",
      "|         4|    Emily|   Brown| Marketing|55000.00|\n",
      "|         5|    David|Williams|   Finance|80000.00|\n",
      "|         6|    Sarah|   Jones|        HR|48000.00|\n",
      "|         7|    Chris|  Taylor|        IT|72000.00|\n",
      "|         8|  Jessica|  Wilson| Marketing|49000.00|\n",
      "+----------+---------+--------+----------+--------+\n",
      "\n",
      "+----------+---------+--------+----------+--------+-------+\n",
      "|EmployeeID|FirstName|LastName|Department|  Salary|grading|\n",
      "+----------+---------+--------+----------+--------+-------+\n",
      "|         1|     John|     Doe|   Finance|75000.00|     A1|\n",
      "|         2|     Jane|   Smith|        HR|60000.00|     A2|\n",
      "|         3|  Michael| Johnson|        IT|45000.00|     A3|\n",
      "|         4|    Emily|   Brown| Marketing|55000.00|     A2|\n",
      "|         5|    David|Williams|   Finance|80000.00|     A1|\n",
      "|         6|    Sarah|   Jones|        HR|48000.00|     A3|\n",
      "|         7|    Chris|  Taylor|        IT|72000.00|     A2|\n",
      "|         8|  Jessica|  Wilson| Marketing|49000.00|     A3|\n",
      "+----------+---------+--------+----------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10 Write a SQL query to classify employees into three categories based on their salary:employees10\n",
    "employees10 = spark.read.format(\"jdbc\").option(\"url\", jdbc_url).option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", 'employees10').option(\"user\", mysql_username).option(\"password\", mysql_password).load()\n",
    "from pyspark.sql.functions import when,count,lit\n",
    "employees10.show()\n",
    "employees10.withColumn(\"grading\",\n",
    "                       when(col('Salary') >=  75000,\"A1\")\n",
    "                       .when((col('Salary')>= 50000) & (col('Salary') < 75000),\"A2\")\n",
    "                       .otherwise(\"A3\")\n",
    "                       ).show()\n",
    "                        \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+----------+--------+\n",
      "|order_id|customer_id|order_date|product_id|quantity|\n",
      "+--------+-----------+----------+----------+--------+\n",
      "|    1001|       C001|2023-01-15|      P001|       4|\n",
      "|    1002|       C001|2023-02-20|      P002|       3|\n",
      "|    1003|       C002|2023-03-10|      P003|       8|\n",
      "|    1004|       C003|2023-04-05|      P004|       2|\n",
      "|    1005|       C004|2023-05-20|      P005|       3|\n",
      "|    1006|       C002|2023-06-15|      P001|       6|\n",
      "|    1007|       C003|2023-07-20|      P002|       1|\n",
      "|    1008|       C004|2023-08-10|      P003|       2|\n",
      "|    1009|       C005|2023-09-05|      P002|       3|\n",
      "|    1010|       C001|2023-10-20|      P002|       1|\n",
      "+--------+-----------+----------+----------+--------+\n",
      "\n",
      "+---------+--------+\n",
      "|return_id|order_id|\n",
      "+---------+--------+\n",
      "|     R001|    1001|\n",
      "|     R002|    1002|\n",
      "|     R003|    1005|\n",
      "|     R004|    1008|\n",
      "|     R005|    1007|\n",
      "+---------+--------+\n",
      "\n",
      "+-----------+------------+-------------+\n",
      "|customer_id|orders_count|customer_type|\n",
      "+-----------+------------+-------------+\n",
      "|       C003|           1|          new|\n",
      "|       C004|           2|    returning|\n",
      "|       C001|           2|    returning|\n",
      "+-----------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 11 Identify returning customers based on their order history. Categorize customers as Returning if they have placed more than one return,  and as New otherwise. \n",
    "orders11 = spark.read.format(\"jdbc\").option(\"url\", jdbc_url).option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", 'orders11').option(\"user\", mysql_username).option(\"password\", mysql_password).load()\n",
    "returns11 = spark.read.format(\"jdbc\").option(\"url\", jdbc_url).option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", 'returns11').option(\"user\", mysql_username).option(\"password\", mysql_password).load()\n",
    "orders11.show()\n",
    "returns11.show()\n",
    "\n",
    "customers_count=orders11.join(returns11,'order_id','inner').groupby('customer_id').agg(count(\"*\").alias(\"orders_count\"))\n",
    "customers_count.withColumn(\"customer_type\",when(col(\"orders_count\")>1,lit(\"returning\")).otherwise(lit(\"new\")) ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------+\n",
      "|emp_id|          name|manager_id|\n",
      "+------+--------------+----------+\n",
      "|     1|      John Doe|      NULL|\n",
      "|     2|    Jane Smith|         1|\n",
      "|     3| Alice Johnson|         1|\n",
      "|     4|     Bob Brown|         3|\n",
      "|     5|   Emily White|      NULL|\n",
      "|     6|   Michael Lee|         3|\n",
      "|     7|   David Clark|      NULL|\n",
      "|     8|   Sarah Davis|         2|\n",
      "|     9|  Kevin Wilson|         2|\n",
      "|    10|Laura Martinez|         4|\n",
      "+------+--------------+----------+\n",
      "\n",
      "+------+--------------+----------+-------------+\n",
      "|emp_id|          name|manager_id| manager_name|\n",
      "+------+--------------+----------+-------------+\n",
      "|     1|      John Doe|      NULL|         NULL|\n",
      "|     5|   Emily White|      NULL|         NULL|\n",
      "|     7|   David Clark|      NULL|         NULL|\n",
      "|     2|    Jane Smith|         1|     John Doe|\n",
      "|     3| Alice Johnson|         1|     John Doe|\n",
      "|     4|     Bob Brown|         3|Alice Johnson|\n",
      "|     6|   Michael Lee|         3|Alice Johnson|\n",
      "|    10|Laura Martinez|         4|    Bob Brown|\n",
      "|     8|   Sarah Davis|         2|   Jane Smith|\n",
      "|     9|  Kevin Wilson|         2|   Jane Smith|\n",
      "+------+--------------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 13 write a SQL query to retrieve all employees  details along with their managers names based on the manager ID\n",
    "employees13 = spark.read.format(\"jdbc\").option(\"url\", jdbc_url).option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", 'employees13').option(\"user\", mysql_username).option(\"password\", mysql_password).load()\n",
    "employees13.show()\n",
    "\n",
    "joined_df = employees13.alias(\"e\").join(\n",
    "    employees13.alias(\"m\"), \n",
    "    col(\"e.manager_id\") == col(\"m.emp_id\"), \n",
    "    \"left\"\n",
    ").select(\n",
    "    col(\"e.*\"), \n",
    "    col(\"m.name\").alias(\"manager_name\")  # Renaming manager's name\n",
    ")\n",
    "\n",
    "# Show result\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------------+\n",
      "|order_id|customer_id|order_date|order_amount|\n",
      "+--------+-----------+----------+------------+\n",
      "|       1|          1|2024-01-03|       50.00|\n",
      "|       2|          2|2024-01-05|       75.00|\n",
      "|       3|          1|2024-01-10|       25.00|\n",
      "|       4|          3|2024-01-15|       60.00|\n",
      "|       5|          2|2024-01-20|       50.00|\n",
      "|       6|          1|2024-02-01|      100.00|\n",
      "|       7|          2|2024-02-05|       25.00|\n",
      "|       8|          3|2024-02-10|       90.00|\n",
      "|       9|          1|2024-02-15|       50.00|\n",
      "|      10|          2|2024-02-20|       75.00|\n",
      "+--------+-----------+----------+------------+\n",
      "\n",
      "+-----------+-------------+-----------------+\n",
      "|customer_id|customer_name|   customer_email|\n",
      "+-----------+-------------+-----------------+\n",
      "|          1|     John Doe| john@example.com|\n",
      "|          2|   Jane Smith| jane@example.com|\n",
      "|          3|Alice Johnson|alice@example.com|\n",
      "|          4|    Bob Brown|  bob@example.com|\n",
      "+-----------+-------------+-----------------+\n",
      "\n",
      "+-----------+-------------+-----------------+--------+----------+------------+\n",
      "|customer_id|customer_name|   customer_email|order_id|order_date|order_amount|\n",
      "+-----------+-------------+-----------------+--------+----------+------------+\n",
      "|          1|     John Doe| john@example.com|       1|2024-01-03|       50.00|\n",
      "|          1|     John Doe| john@example.com|       3|2024-01-10|       25.00|\n",
      "|          1|     John Doe| john@example.com|       6|2024-02-01|      100.00|\n",
      "|          1|     John Doe| john@example.com|       9|2024-02-15|       50.00|\n",
      "|          2|   Jane Smith| jane@example.com|       2|2024-01-05|       75.00|\n",
      "|          2|   Jane Smith| jane@example.com|       5|2024-01-20|       50.00|\n",
      "|          2|   Jane Smith| jane@example.com|       7|2024-02-05|       25.00|\n",
      "|          2|   Jane Smith| jane@example.com|      10|2024-02-20|       75.00|\n",
      "|          3|Alice Johnson|alice@example.com|       4|2024-01-15|       60.00|\n",
      "|          3|Alice Johnson|alice@example.com|       8|2024-02-10|       90.00|\n",
      "+-----------+-------------+-----------------+--------+----------+------------+\n",
      "\n",
      "+-----------+-------------+-----------------+\n",
      "|customer_id|customer_name|sum(order_amount)|\n",
      "+-----------+-------------+-----------------+\n",
      "|          1|     John Doe|           225.00|\n",
      "|          2|   Jane Smith|           225.00|\n",
      "|          3|Alice Johnson|           150.00|\n",
      "+-----------+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 14 Find the top 2 customers who have spent the most money across all their orders.\n",
    "orders14 = spark.read.format(\"jdbc\").option(\"url\", jdbc_url).option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", 'orders14').option(\"user\", mysql_username).option(\"password\", mysql_password).load()\n",
    "customers14 = spark.read.format(\"jdbc\").option(\"url\", jdbc_url).option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", 'customers14').option(\"user\", mysql_username).option(\"password\", mysql_password).load()\n",
    "orders14.show()\n",
    "customers14.show()\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "join_df=customers14.join(orders14,'customer_id','inner')\n",
    "join_df.show()\n",
    "join_df.groupBy('customer_id','customer_name').agg(sum(col('order_amount'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+--------+-----+\n",
      "|order_id|order_date|product_id|quantity|price|\n",
      "+--------+----------+----------+--------+-----+\n",
      "|       1|2024-04-01|         1|      10|50.00|\n",
      "|       2|2024-04-02|         2|       8|40.00|\n",
      "|       3|2024-04-03|         3|      15|30.00|\n",
      "|       4|2024-04-04|         4|      12|25.00|\n",
      "|       5|2024-04-05|         5|       5|60.00|\n",
      "|       6|2024-04-06|         6|      20|20.00|\n",
      "|       7|2024-04-07|         7|      18|35.00|\n",
      "|       8|2024-04-08|         8|      14|45.00|\n",
      "|       9|2024-04-09|         1|      10|50.00|\n",
      "|      10|2024-04-10|         2|       8|40.00|\n",
      "|      11|2024-03-01|         1|      12|50.00|\n",
      "|      12|2024-03-02|         2|      10|40.00|\n",
      "|      13|2024-03-03|         3|      18|30.00|\n",
      "|      14|2024-03-04|         4|      14|25.00|\n",
      "|      15|2024-03-05|         5|       7|60.00|\n",
      "|      16|2024-03-06|         6|      22|20.00|\n",
      "|      17|2024-03-07|         7|      20|35.00|\n",
      "|      18|2024-03-08|         8|      16|45.00|\n",
      "|      19|2024-03-09|         1|      12|50.00|\n",
      "|      20|2024-03-10|         2|      10|40.00|\n",
      "+--------+----------+----------+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+-----------------+----------------+\n",
      "|product_id|prev_net_quantity|prev_net_revenue|\n",
      "+----------+-----------------+----------------+\n",
      "|         6|               51|         1020.00|\n",
      "|         5|               41|         2460.00|\n",
      "|         9|               20|          900.00|\n",
      "|         4|               72|         1800.00|\n",
      "|         7|               34|         3230.00|\n",
      "+----------+-----------------+----------------+\n",
      "\n",
      "+----------+--------------------+-------------------+\n",
      "|product_id|current_net_quantity|current_net_revenue|\n",
      "+----------+--------------------+-------------------+\n",
      "|         6|                  21|             420.00|\n",
      "|         5|                  11|             660.00|\n",
      "|         8|                  18|             810.00|\n",
      "|         7|                  24|             840.00|\n",
      "+----------+--------------------+-------------------+\n",
      "\n",
      "+----------+--------------------+-------------------+-----------------+----------------+\n",
      "|product_id|current_net_quantity|current_net_revenue|prev_net_quantity|prev_net_revenue|\n",
      "+----------+--------------------+-------------------+-----------------+----------------+\n",
      "|         6|                  21|             420.00|               51|         1020.00|\n",
      "|         5|                  11|             660.00|               41|         2460.00|\n",
      "|         7|                  24|             840.00|               34|         3230.00|\n",
      "+----------+--------------------+-------------------+-----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 15 Write an SQL query to retrieve the product details for items whose revenue decreased compared to the previous month. \n",
    "orders15 = spark.read.format(\"jdbc\").option(\"url\", jdbc_url).option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", 'orders15').option(\"user\", mysql_username).option(\"password\", mysql_password).load()\n",
    "orders15.show()\n",
    "from pyspark.sql.functions import year,current_date,month\n",
    "\n",
    "prev_month_revenue=orders15.filter((year('order_date')==year(current_date()))&(month('order_date')==month(current_date())-1)).groupBy(col('product_id')).agg(sum(col('quantity')).alias('prev_net_quantity'),sum(col('quantity')*col('price')).alias('prev_net_revenue'))\n",
    "prev_month_revenue.show()\n",
    "\n",
    "current_month_revenue=orders15.filter((year('order_date')==year(current_date()))&(month('order_date')==month(current_date()))).groupBy(col('product_id')).agg(sum(col('quantity')).alias('current_net_quantity'),sum(col('quantity')*col('price')).alias('current_net_revenue'))\n",
    "current_month_revenue.show()\n",
    "\n",
    "current_month_revenue.join(prev_month_revenue,'product_id','inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+---------+\n",
      "| id|   name|department|managerId|\n",
      "+---+-------+----------+---------+\n",
      "|101|   John|         A|     NULL|\n",
      "|102|    Dan|         A|      101|\n",
      "|103|  James|         A|      101|\n",
      "|104|    Amy|         A|      101|\n",
      "|105|   Anne|         A|      101|\n",
      "|106|    Ron|         B|      101|\n",
      "|107|Michael|         C|     NULL|\n",
      "|108|  Sarah|         C|      107|\n",
      "|109|  Emily|         C|      107|\n",
      "|110|  Brian|         C|      107|\n",
      "+---+-------+----------+---------+\n",
      "\n",
      "+---------+---------------+\n",
      "|managerId|no_of_employees|\n",
      "+---------+---------------+\n",
      "|      101|              5|\n",
      "|     NULL|              2|\n",
      "|      107|              3|\n",
      "+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 16 Write a SQL query to find the names of managers who have at least five direct reports.Return the result table in any order. \n",
    "\n",
    "employees16 = spark.read.format(\"jdbc\").option(\"url\", jdbc_url).option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", 'employees16').option(\"user\", mysql_username).option(\"password\", mysql_password).load()\n",
    "employees16.show()\n",
    "\n",
    "employees16.groupBy('managerId').agg(count('id').alias('no_of_employees')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|customer_id|customer_name|\n",
      "+-----------+-------------+\n",
      "|          1|        Alice|\n",
      "|          2|          Bob|\n",
      "|          3|      Charlie|\n",
      "|          4|        David|\n",
      "|          5|         Emma|\n",
      "+-----------+-------------+\n",
      "\n",
      "+-----------+-----------+----------------+\n",
      "|purchase_id|customer_id|product_category|\n",
      "+-----------+-----------+----------------+\n",
      "|        101|          1|     Electronics|\n",
      "|        102|          1|           Books|\n",
      "|        103|          1|        Clothing|\n",
      "|        104|          1|     Electronics|\n",
      "|        105|          2|        Clothing|\n",
      "|        106|          1|          Beauty|\n",
      "|        107|          3|     Electronics|\n",
      "|        108|          3|           Books|\n",
      "|        109|          4|           Books|\n",
      "|        110|          4|        Clothing|\n",
      "|        111|          4|          Beauty|\n",
      "|        112|          5|     Electronics|\n",
      "|        113|          5|           Books|\n",
      "+-----------+-----------+----------------+\n",
      "\n",
      "4\n",
      "+-----------+-----------------------+\n",
      "|customer_id|distinct_category_count|\n",
      "+-----------+-----------------------+\n",
      "|          1|                      4|\n",
      "+-----------+-----------------------+\n",
      "\n",
      "+-----------+-------------+-----------------------+\n",
      "|customer_id|customer_name|distinct_category_count|\n",
      "+-----------+-------------+-----------------------+\n",
      "|          1|        Alice|                      4|\n",
      "|          3|      Charlie|                      2|\n",
      "|          5|         Emma|                      2|\n",
      "|          4|        David|                      3|\n",
      "|          2|          Bob|                      1|\n",
      "+-----------+-------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 17 Write an SQL query to find customers who have made purchases in all product categories. \n",
    "customers17 = spark.read.format(\"jdbc\").option(\"url\", jdbc_url).option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", 'customers17').option(\"user\", mysql_username).option(\"password\", mysql_password).load()\n",
    "\n",
    "purchases17 = spark.read.format(\"jdbc\").option(\"url\", jdbc_url).option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", 'purchases17').option(\"user\", mysql_username).option(\"password\", mysql_password).load()\n",
    "customers17.show()\n",
    "\n",
    "purchases17.show()\n",
    "\n",
    "from pyspark.sql.functions import countDistinct\n",
    "total_products=purchases17.select(countDistinct('product_category')).collect()[0][0]\n",
    "print(total_products)\n",
    "type(total_products)\n",
    "grouped_df=purchases17.groupBy('customer_id').agg(countDistinct('product_category').alias('distinct_category_count'))\n",
    "filtered_df=grouped_df.filter(col('distinct_category_count')==total_products)\n",
    "customers17.join(filtered_df,'customer_id','inner').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
